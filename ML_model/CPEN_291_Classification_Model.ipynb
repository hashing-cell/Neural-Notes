{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPEN 291 Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrKy6xY7B6j5"
      },
      "source": [
        "# You can get more datasets here https://repository.cloudlab.zhaw.ch/artifactory/deepscores/archives/2017/\n",
        "\n",
        "!wget https://grfia.dlsi.ua.es/primus/packages/CameraPrIMuS.tgz\n",
        "!wget https://raw.githubusercontent.com/OMR-Research/tf-end-to-end/master/Data/vocabulary_semantic.txt\n",
        "!wget https://raw.githubusercontent.com/OMR-Research/tf-end-to-end/master/Data/train.txt\n",
        "!wget https://raw.githubusercontent.com/OMR-Research/tf-end-to-end/master/Data/test.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Den4MDy-fh"
      },
      "source": [
        "!tar -xvf CameraPrIMuS.tgz \n",
        "#!tar -xvf primusCalvoRizoAppliedSciences2018.tgz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2de8AS6wUCh"
      },
      "source": [
        "import torch, torchvision, PIL, numpy as np\n",
        "import pathlib\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import csv\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import crnn \n",
        "from tqdm.auto import tqdm\n",
        "import pdb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT51mf9Sutvg"
      },
      "source": [
        "class MusicClassificationObject:\n",
        "\n",
        "  def __init__(self, dataset_dir, dataset_filenames, dictionary_path, transform=None, label_transform=None, distortions=False):\n",
        "        self.distortions = distortions\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.transform = transform \n",
        "        self.label_transform = label_transform\n",
        "\n",
        "        dataset_filenames = open(dataset_filenames,'r')\n",
        "        dataset_list = dataset_filenames.read().splitlines()\n",
        "        dataset_filenames.close()\n",
        "\n",
        "        self.current_idx = 0\n",
        "        # Dictionary\n",
        "        self.symbol2index = {}\n",
        "        self.index2symbol = {}\n",
        "            \n",
        "        dict_file = open(dictionary_path,'r')\n",
        "        dict_list = dict_file.read().splitlines()\n",
        "        word_idx = 0\n",
        "        for word in dict_list:\n",
        "          self.symbol2index[word] = word_idx\n",
        "          self.index2symbol[word_idx] = word\n",
        "          word_idx += 1 \n",
        "\n",
        "        dict_file.close()\n",
        "\n",
        "        self.dataset = self.createDataset(dataset_list)\n",
        "        self.vocabulary_size = len(self.symbol2index)\n",
        "  \n",
        "  def createDataset(self, dataset_list):\n",
        "    dataset = []\n",
        "    i = 0\n",
        "    for sample in dataset_list:\n",
        "      sample_filepath = self.dataset_dir + '/' + sample + '/' + sample\n",
        "      sample_semantic = sample_filepath + '.semantic'\n",
        "      sample_gt_file = open(sample_semantic, 'r')\n",
        "      sample_gt_plain = sample_gt_file.readline().split()\n",
        "      sample_gt_file.close()\n",
        "      # (img file name, all the notations sequentially related to the file)\n",
        "      dataset.append((sample_filepath + '.png', [self.symbol2index[lab] for lab in sample_gt_plain]))\n",
        "    return dataset\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    if torch.is_tensor(i):\n",
        "        i = i.item()\n",
        "    imgfn, label = self.dataset[i]\n",
        "    img = PIL.Image.open(imgfn)\n",
        "    if self.transform:\n",
        "        img = self.transform(img)\n",
        "    return (img.type(torch.FloatTensor), torch.tensor(label))\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd_ME7pQ1QiC"
      },
      "source": [
        "img_height = 128\n",
        "xform = torchvision.transforms.Compose([torchvision.transforms.Resize(img_height), torchvision.transforms.ToTensor()])\n",
        "music_dataset = MusicClassificationObject('Corpus', 'train.txt', 'vocabulary_semantic.txt', xform)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpRUit6zPVHC",
        "outputId": "6ff2a072-5993-495c-df7d-43742fda715b"
      },
      "source": [
        "music_dataset[13]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
              " tensor([   1,  222, 1759,  250,    0, 1717, 1717,  402,    0,  820, 1741,  851,\n",
              "         1044,  604, 1623,    0,  820, 1741,  851, 1044,  604, 1623,    0, 1210,\n",
              "         1741, 1232, 1477, 1044,  604,    0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGPEixgWBreX"
      },
      "source": [
        "# Converts the dataset into compatiable batches without resizing the width\n",
        "def getBatch(dataset, start_index, batch_size):\n",
        "  max_image_width = 2153\n",
        "  img_channels = 1  \n",
        "  img_height = 128\n",
        "  batch_images = torch.ones([batch_size, img_channels, img_height, max_image_width]).type(torch.FloatTensor)*0\n",
        "  batch_dataset = []\n",
        "  for i in range(batch_size):\n",
        "    sample = dataset[i + start_index]\n",
        "    batch_images[i, :, :sample[0].shape[1], :sample[0].shape[2]] = sample[0]\n",
        "    batch_dataset.append((batch_images[i], sample[1]))\n",
        "  return batch_dataset"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwha6Q2eAppM",
        "outputId": "dddbfd75-332a-42e4-d558-a3ed16eadc43"
      },
      "source": [
        "batch = getBatch(music_dataset, 0, 16)\n",
        "sample, label = batch[0]\n",
        "print(label)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   1,  229, 1757,  236,    0,  687, 1280, 1303, 1303,  823,    0,  490,\n",
            "         490, 1727, 1727,    0, 1470, 1304,  853,  679,  501,    0,  490, 1599,\n",
            "        1727])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDtm97Rb3wKt"
      },
      "source": [
        "def show_img(sample, title=None):\n",
        "    img, _ = sample\n",
        "    if torch.is_tensor(img):\n",
        "        img = img.cpu().permute(1,2,0)\n",
        "    plt.imshow(img[:,:,0])\n",
        "    if title:\n",
        "        plt.title(title) # add color='w' arg if using a dark background\n",
        "    plt.axis('off')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "id": "UGzjGE1f37RR",
        "outputId": "4ead1081-6d47-4a54-e3d8-88df03f0c2a7"
      },
      "source": [
        "show_img(music_dataset[15])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYh-q4v9oUrv"
      },
      "source": [
        "n_all_det = len(music_dataset)\n",
        "n_used_det = int(1 * n_all_det)\n",
        "n_train_det = int(0.8 * n_used_det)\n",
        "n_test_det = n_used_det - n_train_det\n",
        "rng_det = torch.Generator().manual_seed(291)\n",
        "dataset_train, dataset_test, _ = torch.utils.data.random_split(music_dataset, [n_train_det, n_test_det, n_all_det-n_train_det-n_test_det], rng_det)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtE0jfeF3fzO"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Taken from https://github.com/meijieru/crnn.pytorch\n",
        "\n",
        "class BidirectionalLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, nIn, nHidden, nOut):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "\n",
        "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
        "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
        "\n",
        "    def forward(self, input):\n",
        "        recurrent, _ = self.rnn(input)\n",
        "        T, b, h = recurrent.size()\n",
        "        t_rec = recurrent.view(T * b, h)\n",
        "\n",
        "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
        "        output = output.view(T, b, -1)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False):\n",
        "        super(CRNN, self).__init__()\n",
        "        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
        "\n",
        "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
        "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
        "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
        "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
        "\n",
        "        cnn = nn.Sequential()\n",
        "\n",
        "        def convRelu(i, batchNormalization=False):\n",
        "            nIn = nc if i == 0 else nm[i - 1]\n",
        "            nOut = nm[i]\n",
        "            cnn.add_module('conv{0}'.format(i),\n",
        "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
        "            if batchNormalization:\n",
        "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
        "            if leakyRelu:\n",
        "                cnn.add_module('relu{0}'.format(i),\n",
        "                               nn.LeakyReLU(0.2, inplace=True))\n",
        "            else:\n",
        "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
        "\n",
        "        convRelu(0)\n",
        "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
        "        convRelu(1)\n",
        "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
        "        convRelu(2, True)\n",
        "        convRelu(3)\n",
        "        cnn.add_module('pooling{0}'.format(2),\n",
        "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
        "        convRelu(4, True)\n",
        "        convRelu(5)\n",
        "        cnn.add_module('pooling{0}'.format(3),\n",
        "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
        "        convRelu(6, True)  # 512x1x16\n",
        "\n",
        "        self.cnn = cnn\n",
        "        self.rnn = nn.Sequential(\n",
        "            BidirectionalLSTM(512, nh, nh),\n",
        "            BidirectionalLSTM(nh, nh, nclass))\n",
        "\n",
        "    def forward(self, input):\n",
        "        import pdb\n",
        "        pdb.set_trace()\n",
        "        # conv features\n",
        "        conv = self.cnn(input)\n",
        "        b, c, h, w = conv.size()\n",
        "        assert h == 1, \"the height of conv must be 1\"\n",
        "        conv = conv.squeeze(2)\n",
        "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
        "\n",
        "        # rnn features\n",
        "        output = self.rnn(conv)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ08lFNInBfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdabc8df-529a-4b52-ff8b-8366d6f81954"
      },
      "source": [
        "model_det = CRNN(img_height, 1, music_dataset.vocabulary_size, 256)\n",
        "\n",
        "torch.nn.init.xavier_uniform_(torch.empty(256,256))\n",
        "device = torch.device('cuda:0')\n",
        "model_det.to(device)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (cnn): Sequential(\n",
              "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (relu0): ReLU(inplace=True)\n",
              "    (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu2): ReLU(inplace=True)\n",
              "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (relu3): ReLU(inplace=True)\n",
              "    (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
              "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu4): ReLU(inplace=True)\n",
              "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (relu5): ReLU(inplace=True)\n",
              "    (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
              "    (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
              "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu6): ReLU(inplace=True)\n",
              "  )\n",
              "  (rnn): Sequential(\n",
              "    (0): BidirectionalLSTM(\n",
              "      (rnn): LSTM(512, 256, bidirectional=True)\n",
              "      (embedding): Linear(in_features=512, out_features=256, bias=True)\n",
              "    )\n",
              "    (1): BidirectionalLSTM(\n",
              "      (rnn): LSTM(256, 256, bidirectional=True)\n",
              "      (embedding): Linear(in_features=512, out_features=1781, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT9nZE6N0Sbg"
      },
      "source": [
        "criterion_det = torch.nn.CTCLoss()\n",
        "optimizer_det = torch.optim.SGD(model_det.parameters(), lr=0.01)\n",
        "scheduler_det = torch.optim.lr_scheduler.StepLR(optimizer_det, step_size=5, gamma=0.1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1W9fhhQXsPL"
      },
      "source": [
        "def concatenate_labels(labels):\n",
        "    listLabels = []\n",
        "    for x in labels:\n",
        "      listLabels.append(x)\n",
        "    return torch.cat(listLabels, dim=0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bdYyPCM_TmT"
      },
      "source": [
        "def run_test_det(model, criterion, no_preds=False):\n",
        "    nsamples_test = len(dataset_test)\n",
        "    loss = 0\n",
        "    i = 0 \n",
        "    preds = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        while i < nsamples_test:\n",
        "          dataset = getBatch(dataset_test, i, 16)\n",
        "          target_lengths = []\n",
        "          input_lengths = []\n",
        "          labels = []\n",
        "          samples = []\n",
        "          # Here set the target and input lengths for the CTC loss function\n",
        "          for sample, label in dataset: \n",
        "            target_lengths.append(len(label))\n",
        "            input_lengths.append(sample.shape[2])\n",
        "            labels.append(label)\n",
        "            samples.append(sample)\n",
        "          loader_test = torch.utils.data.DataLoader(samples, batch_size=16, shuffle=False)\n",
        "          for data in loader_test:\n",
        "            opt.zero_grad()\n",
        "            samples = data.to(device)\n",
        "            outs = model(samples)\n",
        "            # Set-up the CTC loss function parameters \n",
        "            log_probs = torch.randn(input_length[0], 16, music_dataset.vocabulary_size, dtype=torch.float).log_softmax(2)\n",
        "            loss += criterion(log_probs, labels, input_lengths, target_lengths) * samples.size(0)\n",
        "            if not no_preds:\n",
        "                preds += outs.cpu().unbind()\n",
        "          i += 16\n",
        "    return loss / nsamples_test, preds\n",
        "\n",
        "def run_train_det(model, criterion, opt, sched):\n",
        "    nsamples_train = len(dataset_train)\n",
        "    loss_sofar = 0\n",
        "    i = 0 \n",
        "    model.train()\n",
        "    with torch.enable_grad():\n",
        "        while i < nsamples_train:\n",
        "          dataset = getBatch(dataset_train, i, 16)\n",
        "          target_lengths = []\n",
        "          input_lengths = []\n",
        "          labels = []\n",
        "          samples = []\n",
        "          # Here set the target and input lengths for the CTC loss function\n",
        "          for sample, label in dataset: \n",
        "            target_lengths.append(len(label))\n",
        "            input_lengths.append(sample.shape[2])\n",
        "            labels.append(label)\n",
        "            samples.append(sample)\n",
        "          loader_train = torch.utils.data.DataLoader(samples, batch_size=16, shuffle=False)\n",
        "          for data in loader_train:\n",
        "            opt.zero_grad()\n",
        "            samples = data.to(device)\n",
        "            outs = model(samples)\n",
        "            # Set-up the CTC loss function parameters \n",
        "            log_probs = torch.randn(input_length[0], 16, music_dataset.vocabulary_size, dtype=torch.float).log_softmax(2)\n",
        "            loss = criterion(log_probs, labels, input_lengths, target_lengths)\n",
        "            #loss.backward()\n",
        "            #opt.step()\n",
        "            loss_sofar += loss.item() * samples.size(0)\n",
        "          i += 16\n",
        "    sched.step()\n",
        "    return loss_sofar / nsamples_train\n",
        "\n",
        "def run_all_det(model, criterion, optimizer, scheduler, n_epochs):\n",
        "    for epoch in tqdm(range(n_epochs), desc='epochs'):\n",
        "        loss_train = run_train_det(model, criterion, optimizer, scheduler)\n",
        "        loss_test, _ = run_test_det(model, criterion, no_preds=True)\n",
        "        tqdm.write(f\"epoch {epoch+1}: train loss {loss_train:.4f}, test loss {loss_test:.4f}\")\n",
        "    return loss_test"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwVOPAtc_Xm3"
      },
      "source": [
        "run_all_det(model_det, criterion_det, optimizer_det, scheduler_det, 10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}