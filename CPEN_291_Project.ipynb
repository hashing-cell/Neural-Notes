{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPEN 291 Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKHUqLY7s9AT"
      },
      "source": [
        "import torch, torchvision, PIL, numpy as np\n",
        "import pathlib\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8n0-G6ntiJH"
      },
      "source": [
        "# You can get more datasets here https://repository.cloudlab.zhaw.ch/artifactory/deepscores/archives/2017/\n",
        "!wget https://tuggeluk.github.io/class_names/class_names.csv\n",
        "!wget https://repository.cloudlab.zhaw.ch/artifactory/deepscores/classification/DeepScores2017_classification.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Den4MDy-fh"
      },
      "source": [
        "!unzip DeepScores2017_classification.zip -d music_dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv46qXf13IBn"
      },
      "source": [
        "class ObjectDetectionDataset:\n",
        "    def __init__(self, root_dir, transform=None, transform_label=None):\n",
        "        root_dir = pathlib.Path(root_dir).resolve()\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.transform_label = transform_label\n",
        "        with open(root_dir / 'classes.txt') as f:\n",
        "          # for every line in the data set and strip removes any spaces and leading character\n",
        "            self.classes = [w.strip() for w in f] \n",
        "        # Now split all the dimensions and get their floating point numbers from the file. \n",
        "        def parse_box(line):\n",
        "            kls, cx, cy, sx, sy = line.split()\n",
        "            return int(kls), float(cx), float(cy), float(sx), float(sy)\n",
        "        # opens the file and for each line in the file, it calls parse_box. \n",
        "        def parse_boxes(fn):\n",
        "            with open(fn) as f:\n",
        "                return [parse_box(l) for l in f]\n",
        "        # parse the image file name and the bounding box \n",
        "        self.fns_labels = [(imgfn, parse_boxes(imgfn.with_suffix('.txt')))\n",
        "                           for imgfn in sorted(root_dir.glob('*.jpg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the length of the dataset\n",
        "        return len(self.fns_labels)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if torch.is_tensor(i):\n",
        "            i = i.item()\n",
        "        imgfn, label = self.fns_labels[i]\n",
        "        # read the image from PILLOW library\n",
        "        img = PIL.Image.open(imgfn)\n",
        "        if self.transform:\n",
        "            # apply the transform to the image if it exists!\n",
        "            img = self.transform(img)\n",
        "        if self.transform_label:\n",
        "            # same with the label, transform it! \n",
        "            label = self.transform_label(label)\n",
        "        return (img, label)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}